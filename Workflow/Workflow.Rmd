---
author: "Hector Roux de BÃ©zieux"
date: '`r format(Sys.time(), "%d %B , %Y")`'
title: 'Dune workflow on the baron dataset'
bibliography: workflow.bib
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
  html_document:
    toc: true
    toc_float: TRUE
    toc_depth: 2
    number_sections: true
    code_download: TRUE
---

```{r load packages, include=F}
library(knitr)
opts_chunk$set(fig.pos = "!h", out.extra = "",fig.align = "center",
               fig.height = 6, fig.width = 6)
NCORES <- 2
```

In this workflow, we will demonstrate a full full scRNA-Seq workflow using __Dune__ on an example dataset. We rely on the the data from [@Baron2016], a human pancreas dataset of $8569$ samples. We will demonstrate how to generate various input clustering results, how to merge clusters using __Dune__ and how to select the best output for use in downstream analysis. We will also monitor run times to show the impact of  running __Dune__ versus a workflow without it.


# Load data

We rely on a pre-processed dataset where the count matrix has already been computed, using the __scRNAseq__ R package. The dataset also contains the id of the human donor for each cell, which are used as batch labels. It also contains the cell labels assignements from the original publication. Note that, in that publication, cells were clustered using hierarchical clustering with a final manual merging step. 

```{r, message=F, warning=F}
set.seed(19)
suppressPackageStartupMessages({
  library(SingleCellExperiment)
  library(stringr)
  library(scRNAseq)
})
# Load pre-processed dataset
sce <- BaronPancreasData()
# Filter very lowly expressed genes for computational practices.
filt <- rowSums(counts(sce) >= 2) >= 10
sce <- sce[filt, ]
print(sce)
```

# Pre-processing

Before running clustering algorithms, we will rely on two normalization pipelines. 

+ The default pipeline of __Seurat__ [@Cao2019].

```{r}
suppressPackageStartupMessages({
  library(Seurat)
})
pre_process_time <- system.time({
  se <- CreateSeuratObject(counts = counts(sce),
                           min.cells = 0,
                           min.features = 0,
                           project = "de")
  se <- AddMetaData(se, as.data.frame(colData(sce)))
  se <- NormalizeData(se, verbose = FALSE)
  se <- FindVariableFeatures(se, selection.method = 'vst', nfeatures = 4000,
                             verbose = FALSE)
  se <- se[VariableFeatures(se), ]
  se <- ScaleData(object = se, vars.to.regress = c("nCount_RNA", "donor"))
  sce <- as.SingleCellExperiment(se)
})
```

+ The __scvi__ method [@Lopez2018].

```{r}
# Note that this chunk of code is actually python code run from R. To learn how
# to use the reticulate package, please follow 
# https://rstudio.github.io/reticulate/index.html
suppressPackageStartupMessages({
  library(reticulate)
})
use_python("/usr/local/linux/anaconda3.7/bin/python3")
scvi <- import("scvi")
anndata <- import("anndata")
np <- import("numpy")
sc <- import("scanpy")
scvi_time <- system.time({
  scvi$settings$seed = 0L
  adata <- anndata$AnnData(X = as.sparse(t(counts(sce))),
                           obs = data.frame(cells = colnames(sce),
                                            batch = sce$donor))
  scvi$data$setup_anndata(adata, batch_key = "batch")
  model <- scvi$model$SCVI(adata)
  model$train(n_epochs = 100L, n_epochs_kl_warmup = 25L)
})
```

We can visualize the latent space produced by __scvi__ using the labels from the original publication, and reducing the 10 dimensions of the latent space to 2 using t-SNE [@tsne1, @tsne2, @tsne3].

```{r}
suppressPackageStartupMessages(library(scater))
reducedDim(sce, "scvi") <- model$get_latent_representation()
denoised <- t(model$get_normalized_expression(adata, library_size = 10e4))
dimnames(denoised) <- dimnames(counts(sce))
assay(sce, "denoised") <- log1p(denoised)
sce <- runTSNE(sce, dimred = "scvi")
plotTSNE(sce, colour_by = "label")
```
As we can see, __scvi__ mostly produces a latent space that is consistent with the original labels. Note however that this is information that would not available while analyzing a new dataset. One would instead need to rely on known-marker genes.

# Creating inputs to __Dune__

__Dune__ takes as input a set of clustering results. We will generate a set of such results using a combination of clustering methods and normalization techniques:

+ __SC3__ [@Kiselev2017] using as input the denoised count matrix from __scvi__.
+ __Seurat__ using as input the latent space from __scvi__.
+ __Seurat__ using as input the top pcs from the count matrix normalized using the __Seurat__ pre-processing pipeline.

## __SC3__

__SC3__ is a consensus method that takes as input a normalized count matrix and outputs a set of cluster labels. The __SC3__ package provides a function to estimate the value of $K$, the exact number of clusters, which we will use. 

Since the dataset has more than $5000$ cells, __SC3__ is automatically run in hybrid mode to lower runtime. However, the process can still be quite slow. The code below is run in the default mode. If you want it to run, we recommand seting `default=FALSE`.

```{r, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(SC3))
default <- TRUE
sc3_time <- system.time({
  sce_sc3 <- sce
  logcounts(sce_sc3) <- assay(sce, "denoised")
  rowData(sce_sc3)$feature_symbol <- rownames(sce_sc3)
  counts(sce_sc3) <- as.matrix(counts(sce_sc3))
  logcounts(sce_sc3) <- as.matrix(logcounts(sce_sc3))
  sce_sc3 <- sc3_estimate_k(sce_sc3)
  K <- metadata(sce_sc3)$sc3$k_estimation
  # Note: with R >= 4.0, RStudio and Mac OS, this can fails. 
  # A workaround is running 
  # parallel:::setDefaultClusterOptions(setup_strategy = "sequential")
  if (default) {
    sce_sc3 <- sc3(sce_sc3, ks = K, n_cores = NCORES, rand_seed = 786907)
  } else {
    sce_sc3 <- sc3(sce_sc3, ks = K, n_cores = NCORES, rand_seed = 786907,
                   svm_num_cells = round(.1 * ncol(sce)))
  }
  sce_sc3 <- sc3_run_svm(sce_sc3, ks = K)
  sce$SC3 <- colData(sce_sc3)[, paste0("sc3_", K, "_clusters")] %>% as.factor()
})
plotTSNE(sce, colour_by = "SC3")
```

As we can see, __SC3__ seems to overcluster the data, when compared either to the labels from the original publication, or to the reduced dimension representation. However, this is not a problem since __Dune__ will work better on overclustered results. 

## Seurat 

The second method we use is the clustering algorithm from the __Seurat__ R package, which first construct a Shared Nearest Neighbor (SNN) Graph and then runs the Louvain algorithm on the graph to identify clusters. The SNN graph is build using a reduced dimension representation of the dataset. We first use the default, which is to use the top pcs from the normalized count matrix.

```{r}
seurat_time <- system.time({
  se <- RunPCA(se, verbose = FALSE)
  se <- FindNeighbors(se, verbose = FALSE)
  se <- FindClusters(object = se, verbose = FALSE)
  sce$seurat <- Idents(se)
})
plotTSNE(sce, colour_by = "seurat")
```

__Seurat__ seems to perform better than __SC3__ here but still seems to overpartition the data when run with the default parameters. Once again, it will not be a problem if used as input to __Dune__.


## Seurat with SCVI

Finally, we run the __Seurat__ clustering workflow, but instead of building the SNN using the top 10 pcs, we build it using the latent space from __scvi__. 

```{r}
seurat_scvi_time <- system.time({
  seu <- as.Seurat(x = sce, counts = "counts", data = "counts")
  seu <- FindNeighbors(seu, reduction = "scvi", verbose = FALSE)
  seu <- FindClusters(object = seu, verbose = FALSE)
  sce$seurat_scvi <- Idents(seu)
})
plotTSNE(sce, colour_by = "seurat_scvi")
```

This seems to produce the best result, at least on the latent space of __scvi__, which is not suprising. It also better matches the labels from the original publication but still results in possible over-partition. 

# __Dune__

## Running __Dune__

We can now run __Dune__, using the three clustering results as input. Since all clusterings seem to reflect over-partitioning of the data, __Dune__ will  identify the common underlying structure and polish all inputs, using the Normalized Mutual Information (NMI) as a merging criterion.

```{r}
library(Dune)
df <- colData(sce)[, c("SC3", "seurat", "seurat_scvi")] %>%
  as.matrix()
dune_time <- system.time(
  merger <- Dune(clusMat = df, metric = "NMI")
)
colData(sce)[, c("SC3_final", "seurat_final", "seurat_scvi_final")] <-
  lapply(merger$currentMat, as.factor) %>%
    as.data.frame()
```

## Vizualing the merging

We can first see how the number of clusters in each clustering set decreased as merging occurred, and how the mean NMI increased when merging.

```{r}
NMItrend(merger)
```

# Picking the final clustering result to use

While __Dune__ increases the concordance between the three sets of clusters, it does not pick one at the end. That choice remains up to the user. __Dune__ does not seek to replace biological knowledge or other metrics used to rank clustering methods. Instead, it aims to improve all its inputs, and to lessen the impact of the selection of one set of clusters. 

## Manual selection

One common way to pick clustering results is still manual, using visualization. 

```{r}
plotTSNE(sce, colour_by = "seurat_final")
plotTSNE(sce, colour_by = "seurat_scvi_final")
plotTSNE(sce, colour_by = "SC3_final")
```

Here, we can see that all clustering results look more consistent with the low dimensionality representation. Moreover, it clearly looks like __Seurat__ using the latent space from __scvi__ produces better results. 

## Selection based on sihouette

To provide a more quantitative selection criterion, we can rely on the average silhouette width. This is a number between $-1$ and $1$ that quantify the quality of clustering using the distance matrix between all cells. We compute the distance on the __scvi__ latent space. 

```{r}
library(cluster)
dist_mat <- dist(as.matrix(reducedDim(sce, "scvi")))
sils_init <- lapply(merger$initialMat %>% as.data.frame, function(label){
  silhouette(label, dist = dist_mat)[,3] %>%  mean()
}) %>% unlist()
sils_init
```

This confirm the visual impression: the cluster labels from __Seurat_scvi__ are clearly better on this dataset than the others before merging with __Dune__.

```{r}
sils_final <- lapply(merger$currentMat %>% as.data.frame, function(label){
  silhouette(label, dist = dist_mat)[,3] %>%  mean()
}) %>% unlist()
sils_final
```

For all methods, the average silhouette information increased after merging with __Dune__. Even the best method, __Seurat_scvi__, is greatly improved by the merging. However, the ranking of methods is unchanged: consistent with the visual representation, __Seurat__ using the latent space from __scvi__ clearly outperforms the other two. That is the one that should be used for downstream analysis such as trajectory inference, differential expression or cell type annotation.

## Comparing with the original labels

This last step is not possible on a normal analysis of a new dataset. However, here, we can see how, running all methods using default, we recover cluster labels that match closely clusters from the original publication that had require manual merging using outside biological knowledge.

```{r, warning=FALSE}
suppressPackageStartupMessages({
  library(aricode)
  library(mclust)
})
NMI(sce$label, sce$seurat_scvi_final) %>% round(2)
adjustedRandIndex(sce$label, sce$seurat_scvi_final) %>% round(2)
```

# Runtimes

We can also compare the runtimes of all parts of the workflow. Running __SC3__ in default mode is quite slow, followed by __scvi__. Running __Dune__ itself is quite quick compared to other steps. Using __Dune__ in a workflow increased total runtime but not by orders of magnitudes.

```{r}
times <- c(pre_process_time[1],
           scvi_time[1],
           sc3_time[1],
           seurat_time[1],
           seurat_scvi_time[1],
           dune_time[1])
names(times) <- c("Seurat pre-processing",
                  "SCVI",
                  "SC3",
                  "Seurat",
                  "Seurat after SCVI",
                  "Dune")
df <- data.frame(times = times,
                 Name = factor(names(times), levels = names(times)))
ggplot(df, aes(x = Name, y = times, fill = Name)) +
  geom_col() +
  theme_classic() +
  labs(x = "Step", y = "Time (second)", fill = "Step") +
  scale_fill_brewer(palette = "Dark2") +
  theme(axis.text.x = element_text(angle = 90))
  scale_y_log10()
```

# Session

```{r}
sessionInfo()
```

# References
